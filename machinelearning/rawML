import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
from xgboost import XGBRegressor

data = pd.read_csv("project_risk_data.csv")
data = data.drop(columns=['risk_target_category','magnitude_of_risk','impact','dimension_of_risk','priority'])
new_project = pd.DataFrame({
    'requirements': [300.0],
    'project_category': [2.0],
    'requirement_category': [7.0],
    'probability':[10],
    'affecting_no_of_modules' : [6],
    'fixing_duration' : [12],
    'fix_cost' : [100]
})

data = pd.concat([data, new_project])
test = data.copy()

# probability, priority

encoded = pd.get_dummies(data, columns=["requirements",'project_category','requirement_category'])

y = encoded['risk_score'][:-1]
encoded = encoded.drop(columns=['risk_score'])

X = encoded.iloc[:-1,:]
test = encoded.iloc[-1:,:]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# GB = XGBRegressor(n_estimators=100, learning_rate=0.03, random_state=42)
GB = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
GB.fit(X_train, y_train)

risk_score = GB.predict(test)[0]
print("risk_score", risk_score)

total = GB.feature_importances_[1]+GB.feature_importances_[2]+GB.feature_importances_[3]
ratios = [GB.feature_importances_[1],GB.feature_importances_[2],GB.feature_importances_[3]]
compare = [(ratios[i]/total)*test.iat[0,i+1] for i in range(len(ratios))]

if compare.index(max(compare)) == 0:
    print("Reduce affecting number of modules")
elif compare.index(max(compare)) == 1:
    print("Reduce fixing duration")
else:
    print("Reduce fixing cost")
    
